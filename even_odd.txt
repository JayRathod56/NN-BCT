from random import seed 

 

# Test training backprop algorithm 

seed(1) 

 
 

dataset = [ 

     

[0,0,0,0,0,0,0,0], 

[0,0,0,0,0,0,1,1], 

[0,0,0,0,0,1,0,0], 

[0,0,0,0,0,1,1,1], 

[0,0,0,0,1,0,0,0], 

[0,0,0,0,1,0,1,1], 

[0,0,0,0,1,1,0,0], 

[0,0,0,0,1,1,1,1], 

[0,0,0,1,0,0,0,0], 

[0,0,0,1,0,1,0,1] 

 
 

] 

 

from math import exp 

from random import seed 

from random import random 

 
 

# Initialize a network 

def initialize_network(n_inputs, n_hidden1, n_hidden2, n_outputs): 

  network = list() 

  hidden_layer1 = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden1)] 

  network.append(hidden_layer1) 

  hidden_layer2 = [{'weights':[random() for i in range(n_hidden1 + 1)]} for i in range(n_hidden2)] 

  network.append(hidden_layer2) 

  output_layer = [{'weights':[random() for i in range(n_hidden2 + 1)]} for i in range(n_outputs)] 

  network.append(output_layer) 

  return network 

 
 

# Calculate neuron activation for an input 

def activate(weights, inputs): 

    activation = weights[-1] 

    for i in range(len(weights)-1): 

        activation += weights[i] * inputs[i] 

    return activation 

 
 

# Transfer neuron activation 

def transfer(activation): 

    return 1.0 / (1.0 + exp(-activation)) 

 
 

# Forward propagate input to a network output 

def forward_propagate(network, row): 

    inputs = row 

    for layer in network: 

        new_inputs = [] 

        for neuron in layer: 

            activation = activate(neuron['weights'], inputs) 

            neuron['output'] = transfer(activation) 

            new_inputs.append(neuron['output']) 

        inputs = new_inputs 

    return inputs 

 
 

# Calculate the derivative of an neuron output 

def transfer_derivative(output): 

    return output * (1.0 - output) 

 
 

# Backpropagate error and store in neurons 

def backward_propagate_error(network, expected): 

    for i in reversed(range(len(network))): 

        layer = network[i] 

        errors = list() 

        if i != len(network)-1: 

            for j in range(len(layer)): 

                error = 0.0 

                for neuron in network[i + 1]: 

                    error += (neuron['weights'][j] * neuron['delta']) 

                errors.append(error) 

        else: 

            for j in range(len(layer)): 

                neuron = layer[j] 

                errors.append(neuron['output'] - expected[j]) 

        for j in range(len(layer)): 

            neuron = layer[j] 

            neuron['delta'] = errors[j] * transfer_derivative(neuron['output']) 

 
 

# Update network weights with error 

def update_weights(network, row, l_rate): 

    for i in range(len(network)): 

        inputs = row[:-1] 

        if i != 0: 

            inputs = [neuron['output'] for neuron in network[i - 1]] 

        for neuron in network[i]: 

            for j in range(len(inputs)): 

                neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j] 

            neuron['weights'][-1] -= l_rate * neuron['delta'] 

 
 

# Train a network for a fixed number of epochs 

def train_network(network, train, l_rate, n_epoch, n_outputs): 

    for epoch in range(n_epoch): 

        sum_error = 0 

        for row in train: 

            outputs = forward_propagate(network, row) 

            expected = [0 for i in range(n_outputs)] 

            expected[row[-1]] = 1 

            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))]) 

            backward_propagate_error(network, expected) 

            update_weights(network, row, l_rate) 

        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error)) 

 
 
 

n_inputs = len(dataset[0]) - 1 

n_outputs = len(set([row[-1] for row in dataset])) 

network = initialize_network(n_inputs, 4, 2, n_outputs) 

train_network(network, dataset, 0.5, 500+1, n_outputs) 

for layer in network: 

    print(layer) 

 

from math import exp 

 
 

# Calculate neuron activation for an input 

def activate(weights, inputs): 

    activation = weights[-1] 

    for i in range(len(weights)-1): 

        activation += weights[i] * inputs[i] 

    return activation 

 
 

# Transfer neuron activation 

def transfer(activation): 

    return 1.0 / (1.0 + exp(-activation)) 

 
 

# Forward propagate input to a network output 

def forward_propagate(network, row): 

    inputs = row 

    for layer in network: 

        new_inputs = [] 

        for neuron in layer: 

            activation = activate(neuron['weights'], inputs) 

            neuron['output'] = transfer(activation) 

            new_inputs.append(neuron['output']) 

        inputs = new_inputs 

    return inputs 

 
 

# Make a prediction with a network 

def predict(network, row): 

    outputs = forward_propagate(network, row) 

    return outputs.index(max(outputs)) 

 
 

for row in dataset: 

    prediction = predict(network, row) 

    print('Expected=%d, Got=%d' % (row[-1], prediction)) 

 

en_num = 48 

od_num = 89 

 
 

# convert to one-hot encoding 

 
 

en_num_ohe = [0,1,1,0,0,0,0] 

 
 

od_num_ohe = [1,0,1,1,0,0,1] 

 
 

print("0 --> Even") 

print("1 --> Odd") 

 
 

print(en_num, " : ", predict(network, en_num_ohe)) 

 
 

print(od_num, " : ", predict(network, od_num_ohe)) 

 
 

 
